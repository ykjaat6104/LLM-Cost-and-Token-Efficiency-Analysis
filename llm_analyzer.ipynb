{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a186f5d5",
   "metadata": {},
   "source": [
    "# LLM Cost & Token Efficiency Analyzer\n",
    "> A data-driven benchmarking notebook comparing **all major LLM providers** across cost, latency, and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**Providers Covered:**\n",
    "| Tier | Provider | Models |\n",
    "|------|----------|--------|\n",
    "| Free | **Groq** | llama-3.1-8b-instant, llama-3.3-70b-versatile, llama-4-scout-17b, qwen3-32b |\n",
    "| Free | **Google Gemini** | gemini-1.5-flash, gemini-2.0-flash-exp |\n",
    "| Free | **Cerebras** | llama3.1-8b, llama3.3-70b |\n",
    "| Paid | **OpenAI** | gpt-4o, gpt-4o-mini, gpt-3.5-turbo |\n",
    "| Paid | **Anthropic** | claude-3-5-sonnet, claude-3-haiku |\n",
    "| Paid | **Gemini (paid)** | gemini-1.5-pro |\n",
    "\n",
    "**Sections:**\n",
    "1. Setup & Configuration\n",
    "2. Benchmark Dataset\n",
    "3. Model Runner Function\n",
    "4. Run Experiments\n",
    "5. Visualization & Analysis\n",
    "6. Summary & Recommendations\n",
    "7. RAG Chunk Size vs Cost Experiment\n",
    "\n",
    "> **Currently active tier:** Set `ACTIVE_TIER = \"free\"` to run with free API keys.\n",
    "> Switch to `\"paid\"` or `\"all\"` when you have paid-tier keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d476cb",
   "metadata": {},
   "source": [
    "## Section 1 — Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a05dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies (run once if not already installed)\n",
    "# !pip install groq google-generativeai cerebras-cloud-sdk openai anthropic \\\n",
    "#             pandas numpy matplotlib seaborn tiktoken python-dotenv tenacity\n",
    "\n",
    "# Or install from requirements.txt:\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── Plot styling ──────────────────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': '#0f0f1a',\n",
    "    'axes.facecolor':   '#1a1a2e',\n",
    "    'axes.edgecolor':   '#444466',\n",
    "    'axes.labelcolor':  '#c8c8e8',\n",
    "    'axes.titlecolor':  '#ffffff',\n",
    "    'xtick.color':      '#c8c8e8',\n",
    "    'ytick.color':      '#c8c8e8',\n",
    "    'text.color':       '#c8c8e8',\n",
    "    'grid.color':       '#2a2a4a',\n",
    "    'grid.linestyle':   '--',\n",
    "    'grid.alpha':       0.6,\n",
    "    'font.family':      'DejaVu Sans',\n",
    "    'font.size':        11,\n",
    "})\n",
    "\n",
    "# Extended palette for 14+ models\n",
    "PALETTE = [\n",
    "    '#7c5cbf', '#3aa8c1', '#e84393', '#f5a623', '#50fa7b', '#ff6b6b',\n",
    "    '#bd93f9', '#8be9fd', '#ff79c6', '#ffb86c', '#6272a4', '#44475a',\n",
    "    '#f1fa8c', '#00d4a0',\n",
    "]\n",
    "\n",
    "# Provider brand colors for annotations\n",
    "PROVIDER_COLORS = {\n",
    "    'groq':      '#f84f1d',\n",
    "    'gemini':    '#4285f4',\n",
    "    'cerebras':  '#8c52ff',\n",
    "    'openai':    '#10a37f',\n",
    "    'anthropic': '#cc785c',\n",
    "}\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Run date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52df3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load API Keys from .env ────────────────────────────────────────────────────\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ── FREE TIER keys (get these for free) ───────────────────────────────────────\n",
    "GROQ_API_KEY      = os.getenv(\"GROQ_API_KEY\",      \"\")   # https://console.groq.com/keys\n",
    "GEMINI_API_KEY    = os.getenv(\"GEMINI_API_KEY\",    \"\")   # https://aistudio.google.com/app/apikey\n",
    "CEREBRAS_API_KEY  = os.getenv(\"CEREBRAS_API_KEY\",  \"\")   # https://cloud.cerebras.ai\n",
    "\n",
    "# ── PAID TIER keys (add when you have them) ────────────────────────────────────\n",
    "OPENAI_API_KEY    = os.getenv(\"OPENAI_API_KEY\",    \"\")   # https://platform.openai.com/api-keys\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"\")   # https://console.anthropic.com/settings/keys\n",
    "\n",
    "# ── Tier Selection ─────────────────────────────────────────────────────────────\n",
    "# \"free\"  → only free-tier providers: Groq, Gemini (free), Cerebras\n",
    "# \"paid\"  → only paid providers: OpenAI, Anthropic, Gemini Pro\n",
    "# \"all\"   → run every configured model\n",
    "ACTIVE_TIER = os.getenv(\"ACTIVE_TIER\", \"free\")\n",
    "\n",
    "# ── Demo Mode ─────────────────────────────────────────────────────────────────\n",
    "# True  → simulate API responses (no keys needed, no usage)\n",
    "# False → call real APIs using the keys above\n",
    "DEMO_MODE = os.getenv(\"DEMO_MODE\", \"true\").lower() == \"true\"\n",
    "\n",
    "# ── Status report ─────────────────────────────────────────────────────────────\n",
    "print(f\"Mode        : {'DEMO (simulated)' if DEMO_MODE else 'LIVE (real API)'}\")\n",
    "print(f\"Active tier : {ACTIVE_TIER.upper()}\")\n",
    "print()\n",
    "print(\"Key status:\")\n",
    "print(f\"  GROQ_API_KEY      : {'set' if GROQ_API_KEY     else 'missing'}\")\n",
    "print(f\"  GEMINI_API_KEY    : {'set' if GEMINI_API_KEY   else 'missing'}\")\n",
    "print(f\"  CEREBRAS_API_KEY  : {'set' if CEREBRAS_API_KEY else 'missing'}\")\n",
    "print(f\"  OPENAI_API_KEY    : {'set' if OPENAI_API_KEY   else 'missing'}\")\n",
    "print(f\"  ANTHROPIC_API_KEY : {'set' if ANTHROPIC_API_KEY else 'missing'}\")\n",
    "if not DEMO_MODE:\n",
    "    missing_free = [k for k,v in {\"GROQ\":GROQ_API_KEY,\"GEMINI\":GEMINI_API_KEY,\"CEREBRAS\":CEREBRAS_API_KEY}.items() if not v]\n",
    "    if ACTIVE_TIER in (\"free\",\"all\") and missing_free:\n",
    "        print(f\"\\nWARNING: Missing free-tier keys: {missing_free}. Those models will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model Registry ────────────────────────────────────────────────────────────\n",
    "# provider  : which SDK/API to call\n",
    "# api_id    : exact model ID string sent to the API\n",
    "# tier      : \"free\" or \"paid\"\n",
    "# input/output : cost per token in USD (free-tier = $0.0)\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    # ── FREE TIER: Groq (hardware-accelerated inference) ──────────────────────\n",
    "    \"llama-3.1-8b [Groq]\":     {\"provider\": \"groq\",     \"api_id\": \"llama-3.1-8b-instant\",                    \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    \"llama-3.3-70b [Groq]\":    {\"provider\": \"groq\",     \"api_id\": \"llama-3.3-70b-versatile\",                 \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    \"llama-4-scout [Groq]\":    {\"provider\": \"groq\",     \"api_id\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    \"qwen3-32b [Groq]\":        {\"provider\": \"groq\",     \"api_id\": \"qwen/qwen3-32b\",                          \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    # ── FREE TIER: Google Gemini ───────────────────────────────────────────────\n",
    "    \"gemini-1.5-flash\":        {\"provider\": \"gemini\",   \"api_id\": \"gemini-1.5-flash\",        \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    \"gemini-2.0-flash\":        {\"provider\": \"gemini\",   \"api_id\": \"gemini-2.0-flash-exp\",    \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    # ── FREE TIER: Cerebras (wafer-scale chip, extremely fast) ────────────────\n",
    "    \"llama3.1-8b [Cerebras]\":  {\"provider\": \"cerebras\", \"api_id\": \"llama3.1-8b\",             \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    \"llama3.3-70b [Cerebras]\": {\"provider\": \"cerebras\", \"api_id\": \"llama3.3-70b\",            \"tier\": \"free\", \"input\": 0.0,        \"output\": 0.0},\n",
    "    # ── PAID TIER: OpenAI ─────────────────────────────────────────────────────\n",
    "    \"gpt-4o\":                  {\"provider\": \"openai\",   \"api_id\": \"gpt-4o\",                  \"tier\": \"paid\", \"input\": 5.0/1e6,    \"output\": 15.0/1e6},\n",
    "    \"gpt-4o-mini\":             {\"provider\": \"openai\",   \"api_id\": \"gpt-4o-mini\",             \"tier\": \"paid\", \"input\": 0.15/1e6,   \"output\": 0.6/1e6},\n",
    "    \"gpt-3.5-turbo\":           {\"provider\": \"openai\",   \"api_id\": \"gpt-3.5-turbo\",           \"tier\": \"paid\", \"input\": 0.5/1e6,    \"output\": 1.5/1e6},\n",
    "    # ── PAID TIER: Anthropic ──────────────────────────────────────────────────\n",
    "    \"claude-3-5-sonnet\":       {\"provider\": \"anthropic\",\"api_id\": \"claude-3-5-sonnet-20241022\",\"tier\": \"paid\",\"input\": 3.0/1e6,   \"output\": 15.0/1e6},\n",
    "    \"claude-3-haiku\":          {\"provider\": \"anthropic\",\"api_id\": \"claude-3-haiku-20240307\",  \"tier\": \"paid\", \"input\": 0.25/1e6,  \"output\": 1.25/1e6},\n",
    "    # ── PAID TIER: Gemini Pro ─────────────────────────────────────────────────\n",
    "    \"gemini-1.5-pro\":          {\"provider\": \"gemini\",   \"api_id\": \"gemini-1.5-pro\",          \"tier\": \"paid\", \"input\": 1.25/1e6,   \"output\": 5.0/1e6},\n",
    "}\n",
    "\n",
    "# Backward-compat dict used by calculate_cost()\n",
    "MODEL_PRICING = {name: {\"input\": v[\"input\"], \"output\": v[\"output\"]} for name, v in MODEL_REGISTRY.items()}\n",
    "\n",
    "# ── Task Types ────────────────────────────────────────────────────────────────\n",
    "TASK_TYPES = [\"summarization\", \"qa\", \"rag\", \"classification\", \"code_generation\"]\n",
    "\n",
    "# ── Tier filtering ────────────────────────────────────────────────────────────\n",
    "FREE_MODELS = [k for k, v in MODEL_REGISTRY.items() if v[\"tier\"] == \"free\"]\n",
    "PAID_MODELS = [k for k, v in MODEL_REGISTRY.items() if v[\"tier\"] == \"paid\"]\n",
    "ALL_MODELS  = list(MODEL_REGISTRY.keys())\n",
    "\n",
    "if ACTIVE_TIER == \"free\":\n",
    "    MODELS_TO_TEST = FREE_MODELS\n",
    "elif ACTIVE_TIER == \"paid\":\n",
    "    MODELS_TO_TEST = PAID_MODELS\n",
    "else:\n",
    "    MODELS_TO_TEST = ALL_MODELS\n",
    "\n",
    "print(f\"Configuration complete\")\n",
    "print(f\"Total models in registry : {len(MODEL_REGISTRY)}  ({len(FREE_MODELS)} free, {len(PAID_MODELS)} paid)\")\n",
    "print(f\"Active tier              : {ACTIVE_TIER.upper()} → {len(MODELS_TO_TEST)} models will run\")\n",
    "print(f\"Task types               : {', '.join(TASK_TYPES)}\")\n",
    "print()\n",
    "\n",
    "# Display registry table\n",
    "reg_rows = []\n",
    "for name, v in MODEL_REGISTRY.items():\n",
    "    reg_rows.append({\n",
    "        \"model\": name,\n",
    "        \"provider\": v[\"provider\"],\n",
    "        \"tier\": v[\"tier\"],\n",
    "        \"api_id\": v[\"api_id\"],\n",
    "        \"input ($/1K tok)\":  round(v[\"input\"]  * 1000, 6),\n",
    "        \"output ($/1K tok)\": round(v[\"output\"] * 1000, 6),\n",
    "    })\n",
    "reg_df = pd.DataFrame(reg_rows)\n",
    "reg_df.style.apply(\n",
    "    lambda col: [\"background-color: #1a3a1a\" if t == \"free\" else \"background-color: #2a1a2a\" for t in reg_df[\"tier\"]],\n",
    "    subset=[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a3798",
   "metadata": {},
   "source": [
    "## Section 2 — Benchmark Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de755b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Benchmark Prompts ─────────────────────────────────────────────────────────\n",
    "BENCHMARK_DATASET = {\n",
    "    \"summarization\": [\n",
    "        {\n",
    "            \"prompt\": \"\"\"Summarize the following article in 2-3 sentences:\\n\n",
    "Artificial intelligence has rapidly evolved over the past decade, transforming industries from \n",
    "healthcare to finance. Machine learning models can now diagnose diseases with accuracy rivaling \n",
    "specialists, detect fraud in milliseconds, and generate human-quality text. However, concerns \n",
    "around bias, privacy, and job displacement continue to challenge regulators and companies alike.\"\"\",\n",
    "            \"expected_keywords\": [\"AI\", \"machine learning\", \"healthcare\", \"finance\", \"bias\"]\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"\"\"Summarize this technical concept briefly:\\n\n",
    "Transformer architecture, introduced in 'Attention Is All You Need' (2017), replaced recurrent \n",
    "networks with self-attention mechanisms, enabling parallel processing of sequences. This led to \n",
    "dramatic improvements in NLP tasks and became the foundation for GPT, BERT, and modern LLMs.\"\"\",\n",
    "            \"expected_keywords\": [\"transformer\", \"attention\", \"NLP\", \"parallel\"]\n",
    "        },\n",
    "    ],\n",
    "    \"qa\": [\n",
    "        {\n",
    "            \"prompt\": \"What is the capital of France? Answer in one word.\",\n",
    "            \"expected\": \"Paris\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Who wrote the Python programming language? Answer with the name only.\",\n",
    "            \"expected\": \"Guido van Rossum\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What does REST stand for in API design? Give only the full form.\",\n",
    "            \"expected\": \"Representational State Transfer\"\n",
    "        },\n",
    "    ],\n",
    "    \"rag\": [\n",
    "        {\n",
    "            \"prompt\": \"\"\"Context: Our Q3 2024 earnings report shows revenue of $4.2B, up 18% YoY. \n",
    "Operating margin improved to 23% from 19%. Key growth drivers include cloud services (+34%) \n",
    "and AI products (+52%). Headcount decreased 3% due to efficiency initiatives.\\n\\n\n",
    "Question: What was the YoY revenue growth and what drove it?\"\"\",\n",
    "            \"expected_keywords\": [\"18%\", \"cloud\", \"AI\"]\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"\"\"Context: The company's refund policy states that customers can return \n",
    "products within 30 days for a full refund. Electronics must be unopened. Software licenses \n",
    "are non-refundable after activation. Gift cards cannot be returned.\\n\\n\n",
    "Question: Can I return opened electronics?\"\"\",\n",
    "            \"expected_keywords\": [\"no\", \"unopened\", \"cannot\"]\n",
    "        },\n",
    "    ],\n",
    "    \"classification\": [\n",
    "        {\n",
    "            \"prompt\": \"Classify this customer review as Positive, Negative, or Neutral. Reply with one word only.\\nReview: 'The product arrived on time and works exactly as described. Very happy with my purchase!'\",\n",
    "            \"expected\": \"Positive\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Classify this email as Spam or Not Spam. Reply with one phrase only.\\nEmail: 'Congratulations! You've won $1,000,000. Click here to claim your prize NOW!!!'\",\n",
    "            \"expected\": \"Spam\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Classify the programming language: 'def hello(): print(\\\"Hello World\\\")'. Reply with one word.\",\n",
    "            \"expected\": \"Python\"\n",
    "        },\n",
    "    ],\n",
    "    \"code_generation\": [\n",
    "        {\n",
    "            \"prompt\": \"Write a Python function to calculate the factorial of n using recursion. Include only the function, no explanation.\",\n",
    "            \"expected_keywords\": [\"def\", \"factorial\", \"return\", \"if\"]\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Write a SQL query to find the top 5 customers by total order value. Include only the SQL.\",\n",
    "            \"expected_keywords\": [\"SELECT\", \"ORDER BY\", \"LIMIT\", \"SUM\"]\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "total_prompts = sum(len(v) for v in BENCHMARK_DATASET.values())\n",
    "print(f\"Benchmark dataset loaded: {total_prompts} prompts across {len(BENCHMARK_DATASET)} task types\")\n",
    "for task, prompts in BENCHMARK_DATASET.items():\n",
    "    print(f\" {task:20s}: {len(prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc5db1",
   "metadata": {},
   "source": [
    "## Section 3 — Model Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5abc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Simulated Token Counter (mirrors tiktoken behavior) ───────────────────────\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Rough token estimate: ~4 chars per token (GPT-style tokenization).\"\"\"\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "\n",
    "# ── Cost Calculator ───────────────────────────────────────────────────────────\n",
    "def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"Calculate request cost in USD.\"\"\"\n",
    "    pricing = MODEL_PRICING[model]\n",
    "    cost = (input_tokens * pricing[\"input\"]) + (output_tokens * pricing[\"output\"])\n",
    "    return round(cost, 8)\n",
    "\n",
    "\n",
    "# ── Accuracy Scorer ───────────────────────────────────────────────────────────\n",
    "def score_output(task: str, output: str, benchmark: dict) -> float:\n",
    "    \"\"\"Score model output against expected results. Returns 0.0–1.0.\"\"\"\n",
    "    output_lower = output.lower().strip()\n",
    "    \n",
    "    if \"expected\" in benchmark:\n",
    "        # Exact / partial match for QA and classification\n",
    "        expected = benchmark[\"expected\"].lower()\n",
    "        if expected in output_lower:\n",
    "            return 1.0\n",
    "        # Partial credit: check word overlap\n",
    "        exp_words = set(expected.split())\n",
    "        out_words = set(output_lower.split())\n",
    "        overlap = len(exp_words & out_words) / len(exp_words) if exp_words else 0\n",
    "        return round(overlap, 2)\n",
    "    \n",
    "    elif \"expected_keywords\" in benchmark:\n",
    "        # Keyword coverage for summarization, RAG, code gen\n",
    "        keywords = [kw.lower() for kw in benchmark[\"expected_keywords\"]]\n",
    "        hits = sum(1 for kw in keywords if kw in output_lower)\n",
    "        return round(hits / len(keywords), 2)\n",
    "    \n",
    "    return 0.5  # Default partial score if no benchmark available\n",
    "\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b935409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LIVE API Runners (one per provider) ──────────────────────────────────────\n",
    "\n",
    "def _run_groq(api_id: str, prompt: str) -> dict:\n",
    "    from groq import Groq\n",
    "    client = Groq(api_key=GROQ_API_KEY)\n",
    "    start = time.time()\n",
    "    resp  = client.chat.completions.create(\n",
    "        model=api_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    latency = round(time.time() - start, 3)\n",
    "    return {\n",
    "        \"output\":        resp.choices[0].message.content,\n",
    "        \"input_tokens\":  resp.usage.prompt_tokens,\n",
    "        \"output_tokens\": resp.usage.completion_tokens,\n",
    "        \"latency\":       latency,\n",
    "    }\n",
    "\n",
    "def _run_gemini(api_id: str, prompt: str) -> dict:\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    model_obj = genai.GenerativeModel(api_id)\n",
    "    start = time.time()\n",
    "    resp  = model_obj.generate_content(prompt)\n",
    "    latency = round(time.time() - start, 3)\n",
    "    meta = resp.usage_metadata\n",
    "    return {\n",
    "        \"output\":        resp.text,\n",
    "        \"input_tokens\":  meta.prompt_token_count,\n",
    "        \"output_tokens\": meta.candidates_token_count,\n",
    "        \"latency\":       latency,\n",
    "    }\n",
    "\n",
    "def _run_cerebras(api_id: str, prompt: str) -> dict:\n",
    "    from cerebras.cloud.sdk import Cerebras\n",
    "    client = Cerebras(api_key=CEREBRAS_API_KEY)\n",
    "    start = time.time()\n",
    "    resp  = client.chat.completions.create(\n",
    "        model=api_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    latency = round(time.time() - start, 3)\n",
    "    return {\n",
    "        \"output\":        resp.choices[0].message.content,\n",
    "        \"input_tokens\":  resp.usage.prompt_tokens,\n",
    "        \"output_tokens\": resp.usage.completion_tokens,\n",
    "        \"latency\":       latency,\n",
    "    }\n",
    "\n",
    "def _run_openai(api_id: str, prompt: str) -> dict:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    start = time.time()\n",
    "    resp  = client.chat.completions.create(\n",
    "        model=api_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    latency = round(time.time() - start, 3)\n",
    "    return {\n",
    "        \"output\":        resp.choices[0].message.content,\n",
    "        \"input_tokens\":  resp.usage.prompt_tokens,\n",
    "        \"output_tokens\": resp.usage.completion_tokens,\n",
    "        \"latency\":       latency,\n",
    "    }\n",
    "\n",
    "def _run_anthropic(api_id: str, prompt: str) -> dict:\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "    start = time.time()\n",
    "    resp  = client.messages.create(\n",
    "        model=api_id,\n",
    "        max_tokens=512,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    latency = round(time.time() - start, 3)\n",
    "    return {\n",
    "        \"output\":        resp.content[0].text,\n",
    "        \"input_tokens\":  resp.usage.input_tokens,\n",
    "        \"output_tokens\": resp.usage.output_tokens,\n",
    "        \"latency\":       latency,\n",
    "    }\n",
    "\n",
    "PROVIDER_RUNNERS = {\n",
    "    \"groq\":      _run_groq,\n",
    "    \"gemini\":    _run_gemini,\n",
    "    \"cerebras\":  _run_cerebras,\n",
    "    \"openai\":    _run_openai,\n",
    "    \"anthropic\": _run_anthropic,\n",
    "}\n",
    "\n",
    "def run_model_live(model_name: str, prompt: str) -> dict:\n",
    "    \"\"\"Dispatch to the correct provider runner and attach cost.\"\"\"\n",
    "    reg     = MODEL_REGISTRY[model_name]\n",
    "    runner  = PROVIDER_RUNNERS[reg[\"provider\"]]\n",
    "    result  = runner(reg[\"api_id\"], prompt)\n",
    "    result[\"total_tokens\"] = result[\"input_tokens\"] + result[\"output_tokens\"]\n",
    "    result[\"cost\"]         = calculate_cost(model_name, result[\"input_tokens\"], result[\"output_tokens\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── DEMO Runner (simulated, no API calls) ────────────────────────────────────\n",
    "\n",
    "DEMO_OUTPUTS = {\n",
    "    \"summarization\":   \"AI has transformed industries like healthcare and finance through machine learning, enabling disease diagnosis, fraud detection, and text generation, though bias, privacy, and job concerns remain.\",\n",
    "    \"qa\":              \"Paris\",\n",
    "    \"rag\":             \"Revenue grew 18% YoY, driven primarily by cloud services (+34%) and AI products (+52%).\",\n",
    "    \"classification\":  \"Positive\",\n",
    "    \"code_generation\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\",\n",
    "}\n",
    "\n",
    "# Realistic per-model characteristics: (latency_base_s, latency_std, output_multiplier, accuracy_boost)\n",
    "MODEL_CHARACTERISTICS = {\n",
    "    # Free — Groq (fastest due to LPU hardware)\n",
    "    \"llama-3.1-8b [Groq]\":     {\"latency_base\": 0.30, \"latency_std\": 0.08, \"output_mult\": 0.90, \"accuracy_boost\": 0.00},\n",
    "    \"llama-3.3-70b [Groq]\":    {\"latency_base\": 0.80, \"latency_std\": 0.15, \"output_mult\": 1.05, \"accuracy_boost\": 0.07},\n",
    "    \"llama-4-scout [Groq]\":    {\"latency_base\": 0.55, \"latency_std\": 0.12, \"output_mult\": 1.00, \"accuracy_boost\": 0.08},\n",
    "    \"qwen3-32b [Groq]\":        {\"latency_base\": 0.65, \"latency_std\": 0.12, \"output_mult\": 0.98, \"accuracy_boost\": 0.06},\n",
    "    # Free — Gemini\n",
    "    \"gemini-1.5-flash\":        {\"latency_base\": 0.90, \"latency_std\": 0.20, \"output_mult\": 1.00, \"accuracy_boost\": 0.06},\n",
    "    \"gemini-2.0-flash\":        {\"latency_base\": 0.70, \"latency_std\": 0.15, \"output_mult\": 1.02, \"accuracy_boost\": 0.08},\n",
    "    # Free — Cerebras (wafer-scale, extremely fast)\n",
    "    \"llama3.1-8b [Cerebras]\":  {\"latency_base\": 0.20, \"latency_std\": 0.05, \"output_mult\": 0.88, \"accuracy_boost\": -0.01},\n",
    "    \"llama3.3-70b [Cerebras]\": {\"latency_base\": 0.50, \"latency_std\": 0.10, \"output_mult\": 1.03, \"accuracy_boost\": 0.06},\n",
    "    # Paid — OpenAI\n",
    "    \"gpt-4o\":                  {\"latency_base\": 1.80, \"latency_std\": 0.40, \"output_mult\": 1.10, \"accuracy_boost\": 0.13},\n",
    "    \"gpt-4o-mini\":             {\"latency_base\": 0.90, \"latency_std\": 0.20, \"output_mult\": 0.90, \"accuracy_boost\": 0.07},\n",
    "    \"gpt-3.5-turbo\":           {\"latency_base\": 0.70, \"latency_std\": 0.20, \"output_mult\": 0.85, \"accuracy_boost\": 0.02},\n",
    "    # Paid — Anthropic\n",
    "    \"claude-3-5-sonnet\":       {\"latency_base\": 1.50, \"latency_std\": 0.30, \"output_mult\": 1.15, \"accuracy_boost\": 0.14},\n",
    "    \"claude-3-haiku\":          {\"latency_base\": 0.60, \"latency_std\": 0.15, \"output_mult\": 0.80, \"accuracy_boost\": 0.04},\n",
    "    # Paid — Gemini Pro\n",
    "    \"gemini-1.5-pro\":          {\"latency_base\": 2.00, \"latency_std\": 0.45, \"output_mult\": 1.20, \"accuracy_boost\": 0.10},\n",
    "}\n",
    "\n",
    "BASE_ACCURACY = {\n",
    "    \"summarization\":   0.74,\n",
    "    \"qa\":              0.83,\n",
    "    \"rag\":             0.79,\n",
    "    \"classification\":  0.88,\n",
    "    \"code_generation\": 0.70,\n",
    "}\n",
    "\n",
    "def run_model_demo(model_name: str, prompt: str, task: str) -> dict:\n",
    "    \"\"\"Simulate an API call with realistic per-model variance.\"\"\"\n",
    "    char          = MODEL_CHARACTERISTICS[model_name]\n",
    "    latency       = max(0.15, np.random.normal(char[\"latency_base\"], char[\"latency_std\"]))\n",
    "    input_tokens  = estimate_tokens(prompt)\n",
    "    base_output   = DEMO_OUTPUTS.get(task, \"Sample output.\")\n",
    "    output_tokens = max(1, int(estimate_tokens(base_output) * char[\"output_mult\"] * np.random.uniform(0.85, 1.15)))\n",
    "    cost          = calculate_cost(model_name, input_tokens, output_tokens)\n",
    "    return {\n",
    "        \"output\":          base_output,\n",
    "        \"input_tokens\":    input_tokens,\n",
    "        \"output_tokens\":   output_tokens,\n",
    "        \"total_tokens\":    input_tokens + output_tokens,\n",
    "        \"latency\":         round(latency, 3),\n",
    "        \"cost\":            cost,\n",
    "        \"_accuracy_base\":  BASE_ACCURACY.get(task, 0.75),\n",
    "        \"_accuracy_boost\": char[\"accuracy_boost\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Unified Runner ────────────────────────────────────────────────────────────\n",
    "def run_model(model_name: str, prompt: str, task: str = \"\") -> dict:\n",
    "    \"\"\"Calls real API or demo based on DEMO_MODE.\"\"\"\n",
    "    if DEMO_MODE:\n",
    "        return run_model_demo(model_name, prompt, task)\n",
    "    else:\n",
    "        return run_model_live(model_name, prompt)\n",
    "\n",
    "\n",
    "print(\"Model runner ready — supports: Groq, Gemini, Cerebras, OpenAI, Anthropic\")\n",
    "print(f\"Characteristics loaded for {len(MODEL_CHARACTERISTICS)} models\")\n",
    "print(f\"Demo mode : {DEMO_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af6065",
   "metadata": {},
   "source": [
    "## Section 4 — Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9550c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Key availability check (for live mode) ───────────────────────────────────\n",
    "KEY_MAP = {\n",
    "    \"groq\":      GROQ_API_KEY,\n",
    "    \"gemini\":    GEMINI_API_KEY,\n",
    "    \"cerebras\":  CEREBRAS_API_KEY,\n",
    "    \"openai\":    OPENAI_API_KEY,\n",
    "    \"anthropic\": ANTHROPIC_API_KEY,\n",
    "}\n",
    "\n",
    "def has_key(model_name: str) -> bool:\n",
    "    provider = MODEL_REGISTRY[model_name][\"provider\"]\n",
    "    return bool(KEY_MAP.get(provider, \"\"))\n",
    "\n",
    "# In live mode skip models whose API key is missing; in demo mode run all\n",
    "if DEMO_MODE:\n",
    "    runnable_models = MODELS_TO_TEST\n",
    "else:\n",
    "    runnable_models = [m for m in MODELS_TO_TEST if has_key(m)]\n",
    "    skipped = [m for m in MODELS_TO_TEST if not has_key(m)]\n",
    "    if skipped:\n",
    "        print(f\"Skipping {len(skipped)} models (missing API keys): {skipped}\\n\")\n",
    "\n",
    "# ── Experiment Loop ───────────────────────────────────────────────────────────\n",
    "results   = []\n",
    "run_count = 0\n",
    "total_runs = sum(len(p) for p in BENCHMARK_DATASET.values()) * len(runnable_models)\n",
    "\n",
    "print(f\"Starting experiment\")\n",
    "print(f\"  Models   : {len(runnable_models)}  ({ACTIVE_TIER.upper()} tier)\")\n",
    "print(f\"  Prompts  : {total_prompts}\")\n",
    "print(f\"  Total runs: {total_runs}\")\n",
    "print()\n",
    "\n",
    "for task, prompts in BENCHMARK_DATASET.items():\n",
    "    for i, benchmark in enumerate(prompts):\n",
    "        for model in runnable_models:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                result = run_model(model, benchmark[\"prompt\"], task)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR [{model}] task={task}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Score accuracy\n",
    "            if DEMO_MODE:\n",
    "                base     = result.pop(\"_accuracy_base\", 0.75)\n",
    "                boost    = result.pop(\"_accuracy_boost\", 0.0)\n",
    "                accuracy = min(1.0, max(0.0, base + boost + np.random.normal(0, 0.04)))\n",
    "            else:\n",
    "                accuracy = score_output(task, result[\"output\"], benchmark)\n",
    "\n",
    "            results.append({\n",
    "                \"model\":             model,\n",
    "                \"provider\":          MODEL_REGISTRY[model][\"provider\"],\n",
    "                \"tier\":              MODEL_REGISTRY[model][\"tier\"],\n",
    "                \"task\":              task,\n",
    "                \"prompt_id\":         i,\n",
    "                \"input_tokens\":      result[\"input_tokens\"],\n",
    "                \"output_tokens\":     result[\"output_tokens\"],\n",
    "                \"total_tokens\":      result[\"total_tokens\"],\n",
    "                \"cost\":              result[\"cost\"],\n",
    "                \"latency\":           result[\"latency\"],\n",
    "                \"accuracy\":          round(accuracy, 4),\n",
    "                \"cost_per_accuracy\": round(result[\"cost\"] / max(accuracy, 0.01), 8),\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"Experiment complete — {len(df)} rows\\n\")\n",
    "print(f\"Models run : {df['model'].nunique()}\")\n",
    "print(f\"Providers  : {sorted(df['provider'].unique())}\")\n",
    "print(f\"Tiers      : {df.groupby('tier')['model'].nunique().to_dict()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary Statistics ────────────────────────────────────────────────────────\n",
    "summary = df.groupby([\"model\", \"task\"]).agg(\n",
    "    avg_input_tokens  = (\"input_tokens\",  \"mean\"),\n",
    "    avg_output_tokens = (\"output_tokens\", \"mean\"),\n",
    "    avg_total_tokens  = (\"total_tokens\",  \"mean\"),\n",
    "    avg_cost          = (\"cost\",          \"mean\"),\n",
    "    total_cost        = (\"cost\",          \"sum\"),\n",
    "    avg_latency       = (\"latency\",       \"mean\"),\n",
    "    avg_accuracy      = (\"accuracy\",      \"mean\"),\n",
    ").round(6).reset_index()\n",
    "\n",
    "print(\"Summary Statistics (averaged per model/task):\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97838c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model-level aggregate ─────────────────────────────────────────────────────\n",
    "model_summary = df.groupby(\"model\").agg(\n",
    "    total_tokens  = (\"total_tokens\", \"sum\"),\n",
    "    total_cost    = (\"cost\",         \"sum\"),\n",
    "    avg_latency   = (\"latency\",      \"mean\"),\n",
    "    avg_accuracy  = (\"accuracy\",     \"mean\"),\n",
    ").round(6)\n",
    "model_summary[\"efficiency_score\"] = (model_summary[\"avg_accuracy\"] / model_summary[\"total_cost\"]).round(2)\n",
    "model_summary = model_summary.sort_values(\"avg_accuracy\", ascending=False)\n",
    "\n",
    "print(\"Model Rankings (by average accuracy):\")\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236dbf68",
   "metadata": {},
   "source": [
    "## Section 5 — Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc72e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 1: Token Usage per Model ───────────────────────────────────────────\n",
    "import os\n",
    "OUT_DIR = os.path.join(os.getcwd(), 'outputs')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Token Usage Analysis\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "# Grouped bar: avg token breakdown per model\n",
    "token_data = df.groupby(\"model\")[[\"input_tokens\", \"output_tokens\"]].mean().sort_values(\"input_tokens\")\n",
    "x = np.arange(len(token_data))\n",
    "width = 0.35\n",
    "\n",
    "ax = axes[0]\n",
    "bars1 = ax.bar(x - width/2, token_data[\"input_tokens\"],  width, label=\"Input\",  color=PALETTE[0], alpha=0.9)\n",
    "bars2 = ax.bar(x + width/2, token_data[\"output_tokens\"], width, label=\"Output\", color=PALETTE[1], alpha=0.9)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(token_data.index, rotation=30, ha='right')\n",
    "ax.set_ylabel(\"Avg Tokens per Request\")\n",
    "ax.set_title(\"Input vs Output Tokens per Model\")\n",
    "ax.legend()\n",
    "ax.grid(axis='y')\n",
    "\n",
    "# Stacked bar: avg total tokens per task type\n",
    "task_token = df.groupby([\"model\", \"task\"])[\"total_tokens\"].mean().unstack(fill_value=0)\n",
    "bottom = np.zeros(len(task_token))\n",
    "ax2 = axes[1]\n",
    "for j, task in enumerate(task_token.columns):\n",
    "    ax2.bar(task_token.index, task_token[task], bottom=bottom, label=task, color=PALETTE[j % len(PALETTE)], alpha=0.85)\n",
    "    bottom += task_token[task].values\n",
    "ax2.set_xticklabels(task_token.index, rotation=30, ha='right')\n",
    "ax2.set_ylabel(\"Avg Total Tokens\")\n",
    "ax2.set_title(\"Token Distribution by Task\")\n",
    "ax2.legend(loc='upper left', fontsize=8)\n",
    "ax2.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart1_tokens.png'), dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "print(\"Chart 1 rendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e737bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 2: Latency Comparison ───────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle(\"Latency Analysis\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "models_in_df = list(df[\"model\"].unique())\n",
    "\n",
    "# Box plot of latency distribution\n",
    "ax = axes[0]\n",
    "latency_by_model = [df[df[\"model\"] == m][\"latency\"].values for m in models_in_df]\n",
    "bp = ax.boxplot(latency_by_model, labels=models_in_df, patch_artist=True,\n",
    "                medianprops=dict(color='white', linewidth=2))\n",
    "for patch, color in zip(bp['boxes'], PALETTE):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.8)\n",
    "ax.set_xticklabels(models_in_df, rotation=45, ha='right', fontsize=7)\n",
    "ax.set_ylabel(\"Latency (seconds)\")\n",
    "ax.set_title(\"Latency Distribution per Model\")\n",
    "ax.grid(axis='y')\n",
    "\n",
    "# Heatmap: avg latency per model x task\n",
    "ax2 = axes[1]\n",
    "lat_pivot = df.groupby([\"model\", \"task\"])[\"latency\"].mean().unstack().round(3)\n",
    "sns.heatmap(lat_pivot, ax=ax2, cmap=\"YlOrRd\", annot=True, fmt=\".2f\",\n",
    "            linewidths=0.5, linecolor='#1a1a2e',\n",
    "            cbar_kws={'label': 'Avg Latency (s)'})\n",
    "ax2.set_title(\"Avg Latency Heatmap (Model x Task)\")\n",
    "ax2.set_xlabel(\"Task\")\n",
    "ax2.set_ylabel(\"Model\")\n",
    "ax2.tick_params(axis='x', rotation=30)\n",
    "ax2.tick_params(axis='y', labelsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart2_latency.png'), dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "print(\"Chart 2 rendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 3: Cost per Task & Model ───────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Cost Analysis\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "# Avg cost per model (bar chart)\n",
    "ax = axes[0]\n",
    "cost_per_model = df.groupby(\"model\")[\"cost\"].mean().sort_values(ascending=True) * 1e6  # convert to µ$\n",
    "bars = ax.barh(cost_per_model.index, cost_per_model.values,\n",
    "               color=[PALETTE[i % len(PALETTE)] for i in range(len(cost_per_model))], alpha=0.9)\n",
    "ax.set_xlabel(\"Avg Cost per Request (µ$ = $0.000001)\")\n",
    "ax.set_title(\"Average Cost per Request by Model\")\n",
    "for bar, val in zip(bars, cost_per_model.values):\n",
    "    ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "            f\"{val:.2f}µ$\", va='center', fontsize=9, color='white')\n",
    "ax.grid(axis='x')\n",
    "\n",
    "# Cost breakdown by task (grouped bar)\n",
    "ax2 = axes[1]\n",
    "cost_pivot = df.groupby([\"task\", \"model\"])[\"cost\"].mean().unstack() * 1e6\n",
    "cost_pivot.plot(kind='bar', ax=ax2, color=PALETTE[:len(MODELS_TO_TEST)], alpha=0.85, width=0.75)\n",
    "ax2.set_xticklabels(cost_pivot.index, rotation=30, ha='right')\n",
    "ax2.set_ylabel(\"Avg Cost (µ$)\")\n",
    "ax2.set_title(\"Cost per Task by Model\")\n",
    "ax2.legend(loc='upper right', fontsize=7, ncol=2)\n",
    "ax2.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart3_cost.png'), dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "print(\"Chart 3 rendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 4: Cost vs Accuracy Scatter Plot ───────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle(\"Cost vs Accuracy Trade-Off\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "# Scatter: Overall (aggregated per model)\n",
    "ax = axes[0]\n",
    "agg = df.groupby(\"model\").agg(avg_cost=(\"cost\",\"mean\"), avg_accuracy=(\"accuracy\",\"mean\")).reset_index()\n",
    "agg[\"avg_cost_micro\"] = agg[\"avg_cost\"] * 1e6\n",
    "\n",
    "for i, row in agg.iterrows():\n",
    "    color = PALETTE[i % len(PALETTE)]\n",
    "    ax.scatter(row[\"avg_cost_micro\"], row[\"avg_accuracy\"],\n",
    "               s=250, color=color, zorder=5, edgecolors='white', linewidth=1.5)\n",
    "    ax.annotate(row[\"model\"], (row[\"avg_cost_micro\"], row[\"avg_accuracy\"]),\n",
    "                textcoords=\"offset points\", xytext=(8, 4), fontsize=8, color=color)\n",
    "\n",
    "# Frontier line (Pareto-ish — cheapest path to best accuracy)\n",
    "agg_s = agg.sort_values(\"avg_cost_micro\")\n",
    "ax.plot(agg_s[\"avg_cost_micro\"], agg_s[\"avg_accuracy\"], '--', color='#ffffff33', lw=1.5)\n",
    "\n",
    "ax.set_xlabel(\"Avg Cost per Request (µ$)\")\n",
    "ax.set_ylabel(\"Avg Accuracy Score\")\n",
    "ax.set_title(\"Overall: Cost vs Accuracy\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Add quadrant labels\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "mid_x = (xlim[0] + xlim[1]) / 2\n",
    "mid_y = (ylim[0] + ylim[1]) / 2\n",
    "ax.text(xlim[0]+0.1, ylim[1]-0.01, \"Cheap + Accurate\",   fontsize=7, color='#50fa7b', alpha=0.7)\n",
    "ax.text(xlim[1]*0.6, ylim[1]-0.01, \"Costly + Accurate\",  fontsize=7, color='#f5a623', alpha=0.7)\n",
    "ax.text(xlim[0]+0.1, ylim[0]+0.005,\"Cheap + Inaccurate\", fontsize=7, color='#ff6b6b', alpha=0.7)\n",
    "ax.text(xlim[1]*0.6, ylim[0]+0.005,\"Costly + Inaccurate\",fontsize=7, color='#e84393', alpha=0.7)\n",
    "\n",
    "# Per-task breakdown scatter\n",
    "ax2 = axes[1]\n",
    "task_agg = df.groupby([\"model\", \"task\"]).agg(avg_cost=(\"cost\",\"mean\"), avg_accuracy=(\"accuracy\",\"mean\")).reset_index()\n",
    "task_agg[\"avg_cost_micro\"] = task_agg[\"avg_cost\"] * 1e6\n",
    "\n",
    "task_colors = {t: PALETTE[i] for i, t in enumerate(TASK_TYPES)}\n",
    "for task in TASK_TYPES:\n",
    "    subset = task_agg[task_agg[\"task\"] == task]\n",
    "    ax2.scatter(subset[\"avg_cost_micro\"], subset[\"avg_accuracy\"],\n",
    "                label=task, color=task_colors[task], s=100, alpha=0.8, edgecolors='white', linewidth=0.8)\n",
    "\n",
    "ax2.set_xlabel(\"Avg Cost per Request (µ$)\")\n",
    "ax2.set_ylabel(\"Avg Accuracy Score\")\n",
    "ax2.set_title(\"Cost vs Accuracy by Task Type\")\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart4_scatter.png'), dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "print(\"Chart 4 rendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 5: Efficiency Score (Accuracy per Dollar) ──────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Model Efficiency: Accuracy per Dollar\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "# Efficiency score = accuracy / cost\n",
    "efficiency = df.groupby(\"model\").apply(\n",
    "    lambda g: (g[\"accuracy\"].mean() / max(g[\"cost\"].mean(), 1e-9)),\n",
    "    include_groups=False\n",
    ").sort_values(ascending=True)\n",
    "\n",
    "ax = axes[0]\n",
    "colors = [PALETTE[i % len(PALETTE)] for i in range(len(efficiency))]\n",
    "bars = ax.barh(efficiency.index, efficiency.values, color=colors, alpha=0.9)\n",
    "ax.set_xlabel(\"Efficiency Score (Accuracy / Avg Cost)\")\n",
    "ax.set_title(\"Value for Money: Higher = Better\")\n",
    "for bar, val in zip(bars, efficiency.values):\n",
    "    ax.text(bar.get_width() + efficiency.max()*0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f\"{val:.0f}\", va='center', fontsize=9, color='white')\n",
    "ax.grid(axis='x')\n",
    "\n",
    "# Multi-metric comparison (normalized)\n",
    "ax2 = axes[1]\n",
    "\n",
    "metrics = [\"accuracy\", \"latency\", \"cost\"]\n",
    "metric_labels = [\"Accuracy\", \"Speed\", \"Cheapness\"]\n",
    "model_metrics = df.groupby(\"model\").agg(\n",
    "    accuracy=(\"accuracy\",\"mean\"),\n",
    "    latency=(\"latency\",\"mean\"),\n",
    "    cost=(\"cost\",\"mean\")\n",
    ")\n",
    "# Normalize to 0-1 (higher is always better)\n",
    "norm = model_metrics.copy()\n",
    "norm[\"accuracy\"] = (model_metrics[\"accuracy\"] - model_metrics[\"accuracy\"].min()) / (model_metrics[\"accuracy\"].max() - model_metrics[\"accuracy\"].min() + 1e-9)\n",
    "norm[\"latency\"]  = 1 - (model_metrics[\"latency\"] - model_metrics[\"latency\"].min()) / (model_metrics[\"latency\"].max() - model_metrics[\"latency\"].min() + 1e-9)\n",
    "norm[\"cost\"]     = 1 - (model_metrics[\"cost\"] - model_metrics[\"cost\"].min()) / (model_metrics[\"cost\"].max() - model_metrics[\"cost\"].min() + 1e-9)\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "for i, model in enumerate(norm.index):\n",
    "    ax2.plot(x_pos, norm.loc[model, metrics].values, 'o-',\n",
    "             color=PALETTE[i % len(PALETTE)], label=model, alpha=0.85, lw=2, markersize=7)\n",
    "\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(metric_labels)\n",
    "ax2.set_ylim(-0.05, 1.1)\n",
    "ax2.set_ylabel(\"Normalized Score (higher = better)\")\n",
    "ax2.set_title(\"Multi-Metric Profile Comparison\")\n",
    "ax2.legend(loc='lower right', fontsize=8)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart5_efficiency.png'), dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "print(\"Chart 5 rendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 6: Accuracy Heatmap by Model x Task ─────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "fig.suptitle(\"Accuracy Heatmap: Model x Task\", fontsize=15, fontweight='bold', color='white')\n",
    "\n",
    "acc_pivot = df.groupby([\"model\", \"task\"])[\"accuracy\"].mean().unstack().round(3)\n",
    "mask = acc_pivot.isnull()\n",
    "sns.heatmap(acc_pivot, ax=ax, cmap=\"RdYlGn\", vmin=0.5, vmax=1.0,\n",
    "            annot=True, fmt=\".2f\", linewidths=0.5, linecolor='#0f0f1a',\n",
    "            cbar_kws={'label': 'Accuracy Score'},\n",
    "            mask=mask)\n",
    "ax.set_xlabel(\"Task Type\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart6_accuracy_heatmap.png'), dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "plt.show()\n",
    "print(\"Chart 6 rendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Chart 7: Free vs Paid Tier Direct Comparison ─────────────────────────────\n",
    "tiers_present = df[\"tier\"].unique()\n",
    "if len(tiers_present) < 2:\n",
    "    print(f\"Only '{tiers_present[0]}' tier data present — skipping tier comparison chart.\")\n",
    "    print(\"Set ACTIVE_TIER = 'all' and re-run to see both tiers.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig.suptitle(\"Free vs Paid Tier Comparison\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "    tier_agg = df.groupby([\"model\", \"tier\"]).agg(\n",
    "        avg_accuracy  = (\"accuracy\",  \"mean\"),\n",
    "        avg_latency   = (\"latency\",   \"mean\"),\n",
    "        avg_cost_micro= (\"cost\",      lambda x: x.mean() * 1e6),\n",
    "    ).reset_index()\n",
    "\n",
    "    tier_colors = {\"free\": \"#50fa7b\", \"paid\": \"#ff79c6\"}\n",
    "\n",
    "    # Panel 1: Accuracy by model, coloured by tier\n",
    "    ax = axes[0]\n",
    "    tier_agg_s = tier_agg.sort_values(\"avg_accuracy\", ascending=True)\n",
    "    bar_colors = [tier_colors[t] for t in tier_agg_s[\"tier\"]]\n",
    "    ax.barh(tier_agg_s[\"model\"], tier_agg_s[\"avg_accuracy\"], color=bar_colors, alpha=0.85)\n",
    "    ax.set_xlabel(\"Avg Accuracy\")\n",
    "    ax.set_title(\"Accuracy — Free vs Paid\")\n",
    "    ax.tick_params(axis='y', labelsize=7)\n",
    "    ax.grid(axis='x')\n",
    "    patches = [mpatches.Patch(color=c, label=t.capitalize()) for t, c in tier_colors.items() if t in tiers_present]\n",
    "    ax.legend(handles=patches, loc='lower right', fontsize=8)\n",
    "\n",
    "    # Panel 2: Latency by model\n",
    "    ax2 = axes[1]\n",
    "    tier_agg_s2 = tier_agg.sort_values(\"avg_latency\", ascending=True)\n",
    "    bar_colors2 = [tier_colors[t] for t in tier_agg_s2[\"tier\"]]\n",
    "    ax2.barh(tier_agg_s2[\"model\"], tier_agg_s2[\"avg_latency\"], color=bar_colors2, alpha=0.85)\n",
    "    ax2.set_xlabel(\"Avg Latency (s)\")\n",
    "    ax2.set_title(\"Latency — Free vs Paid\")\n",
    "    ax2.tick_params(axis='y', labelsize=7)\n",
    "    ax2.grid(axis='x')\n",
    "\n",
    "    # Panel 3: Scatter accuracy vs latency, sized by cost\n",
    "    ax3 = axes[2]\n",
    "    for _, row in tier_agg.iterrows():\n",
    "        color = tier_colors.get(row[\"tier\"], \"#ffffff\")\n",
    "        size  = max(50, min(500, (row[\"avg_cost_micro\"] + 0.5) * 80))\n",
    "        ax3.scatter(row[\"avg_latency\"], row[\"avg_accuracy\"],\n",
    "                    s=size, color=color, alpha=0.85,\n",
    "                    edgecolors='white', linewidth=0.8, zorder=5)\n",
    "        ax3.annotate(row[\"model\"], (row[\"avg_latency\"], row[\"avg_accuracy\"]),\n",
    "                     textcoords=\"offset points\", xytext=(5, 3), fontsize=6, color=color)\n",
    "    ax3.set_xlabel(\"Avg Latency (s)\")\n",
    "    ax3.set_ylabel(\"Avg Accuracy\")\n",
    "    ax3.set_title(\"Accuracy vs Latency\\n(bubble size = cost)\")\n",
    "    ax3.grid(True)\n",
    "    ax3.legend(handles=patches, loc='lower right', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, 'chart7_tier_comparison.png'), dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "    plt.show()\n",
    "    print(\"Chart 7 rendered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e8d68",
   "metadata": {},
   "source": [
    "## Section 6 — Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Final Rankings Table ───────────────────────────────────────────────────────\n",
    "final_report = df.groupby([\"model\", \"provider\", \"tier\"]).agg(\n",
    "    avg_accuracy   = (\"accuracy\",     \"mean\"),\n",
    "    avg_latency_s  = (\"latency\",      \"mean\"),\n",
    "    avg_cost_micro = (\"cost\",         lambda x: x.mean() * 1e6),\n",
    "    total_cost_usd = (\"cost\",         \"sum\"),\n",
    "    total_tokens   = (\"total_tokens\", \"sum\"),\n",
    ").round(4).reset_index()\n",
    "\n",
    "final_report[\"efficiency\"]    = (final_report[\"avg_accuracy\"] / (final_report[\"avg_cost_micro\"] + 0.001)).round(2)\n",
    "final_report[\"rank_accuracy\"] = final_report[\"avg_accuracy\"].rank(ascending=False).astype(int)\n",
    "final_report[\"rank_cost\"]     = final_report[\"avg_cost_micro\"].rank(ascending=True).astype(int)\n",
    "final_report[\"rank_speed\"]    = final_report[\"avg_latency_s\"].rank(ascending=True).astype(int)\n",
    "final_report = final_report.sort_values(\"avg_accuracy\", ascending=False).set_index(\"model\")\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"FINAL MODEL BENCHMARK REPORT\")\n",
    "print(\"=\" * 75)\n",
    "cols = [\"provider\",\"tier\",\"avg_accuracy\",\"avg_latency_s\",\"avg_cost_micro\",\"efficiency\",\"rank_accuracy\",\"rank_speed\",\"rank_cost\"]\n",
    "print(final_report[cols].to_string())\n",
    "print()\n",
    "\n",
    "# ── Overall winners ────────────────────────────────────────────────────────────\n",
    "best_accuracy  = final_report[\"avg_accuracy\"].idxmax()\n",
    "best_speed     = final_report[\"avg_latency_s\"].idxmin()\n",
    "best_efficient = final_report[\"efficiency\"].idxmax()\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print(\"OVERALL WINNERS\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"Best Accuracy   : {best_accuracy}\")\n",
    "print(f\"Fastest Model   : {best_speed}\")\n",
    "print(f\"Best Value      : {best_efficient}  (accuracy / cost)\")\n",
    "\n",
    "# ── Free tier winners ──────────────────────────────────────────────────────────\n",
    "free_df = final_report[final_report[\"tier\"] == \"free\"]\n",
    "if not free_df.empty:\n",
    "    print()\n",
    "    print(\"-\" * 55)\n",
    "    print(\"FREE TIER WINNERS\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"Best Accuracy   : {free_df['avg_accuracy'].idxmax()}\")\n",
    "    print(f\"Fastest Model   : {free_df['avg_latency_s'].idxmin()}\")\n",
    "    print(f\"Best Value      : {free_df['efficiency'].idxmax()}\")\n",
    "\n",
    "# ── Paid tier winners ──────────────────────────────────────────────────────────\n",
    "paid_df = final_report[final_report[\"tier\"] == \"paid\"]\n",
    "if not paid_df.empty:\n",
    "    best_cheap_paid = paid_df[\"avg_cost_micro\"].idxmin()\n",
    "    print()\n",
    "    print(\"-\" * 55)\n",
    "    print(\"PAID TIER WINNERS\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"Best Accuracy   : {paid_df['avg_accuracy'].idxmax()}\")\n",
    "    print(f\"Most Affordable : {best_cheap_paid}\")\n",
    "    print(f\"Best Value      : {paid_df['efficiency'].idxmax()}\")\n",
    "\n",
    "# ── Use case guide ─────────────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"-\" * 55)\n",
    "print(\"USE CASE GUIDE\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"No budget / prototyping       : {free_df['avg_accuracy'].idxmax() if not free_df.empty else 'N/A'}\")\n",
    "print(f\"Fastest response needed       : {best_speed}\")\n",
    "print(f\"High-stakes tasks             : {best_accuracy}\")\n",
    "print(f\"High-volume production        : {paid_df['avg_cost_micro'].idxmin() if not paid_df.empty else best_efficient}\")\n",
    "print(f\"Balanced (accuracy + cost)    : {best_efficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Export Results ─────────────────────────────────────────────────────────────\n",
    "df.to_csv(os.path.join(OUT_DIR, 'llm_benchmark_results.csv'), index=False)\n",
    "final_report.to_csv(os.path.join(OUT_DIR, 'llm_benchmark_summary.csv'))\n",
    "\n",
    "print(\"Results exported:\")\n",
    "print(f\" {os.path.join(OUT_DIR, 'llm_benchmark_results.csv')}  — Full run-level data\")\n",
    "print(f\" {os.path.join(OUT_DIR, 'llm_benchmark_summary.csv')}  — Model-level summary\")\n",
    "print()\n",
    "print(f\"Total records in dataset : {len(df)}\")\n",
    "print(f\"Total simulated spend    : ${df['cost'].sum():.6f} USD\")\n",
    "print(f\"Total tokens processed   : {df['total_tokens'].sum():,}\")\n",
    "print(f\"Total simulated latency  : {df['latency'].sum():.1f} seconds\")\n",
    "print()\n",
    "print(\"Benchmark complete! Scroll up to review charts & recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6426bc",
   "metadata": {},
   "source": [
    "## Section 7 — RAG Chunk Size vs Cost Experiment\n",
    "\n",
    "> **What this tests:** How chunk size in retrieval affects total context tokens, answer accuracy, and cost per query.\n",
    ">\n",
    "> Chunk sizes tested: **200 / 500 / 1000 tokens**\n",
    ">\n",
    "> For each chunk size, we simulate retrieving top-k chunks and measure input tokens, cost, and accuracy.\n",
    "> This reveals the optimal chunk size sweet spot — large chunks increase cost, small chunks risk losing context quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e450d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RAG Experiment Setup ──────────────────────────────────────────────────────\n",
    "\n",
    "# Long document to chunk (realistic enterprise knowledge base article)\n",
    "SOURCE_DOCUMENT = \"\"\"\n",
    "Artificial intelligence (AI) is transforming industries at an unprecedented pace. In healthcare,\n",
    "AI systems can now detect diseases from medical imaging with accuracy that rivals experienced\n",
    "specialists. In finance, machine learning models process millions of transactions per second to\n",
    "identify fraud patterns that humans would miss. In manufacturing, predictive maintenance powered\n",
    "by AI reduces equipment downtime by up to 40%.\n",
    "\n",
    "The transformer architecture, introduced in the landmark paper \"Attention Is All You Need\" (2017),\n",
    "replaced recurrent neural networks with self-attention mechanisms. This allowed parallel processing\n",
    "of entire sequences, dramatically speeding up training times and enabling models to capture\n",
    "long-range dependencies in text. The architecture became the foundation for GPT, BERT, T5, and\n",
    "virtually all modern large language models.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. When a\n",
    "question is asked, relevant documents are first retrieved from a knowledge base, then passed as\n",
    "context to the LLM to generate a grounded answer. This reduces hallucination, improves factual\n",
    "accuracy, and allows the model to answer questions about private or recent data without retraining.\n",
    "\n",
    "Prompt engineering is the practice of designing inputs to language models to elicit desired outputs.\n",
    "Techniques include zero-shot prompting, few-shot prompting with examples, chain-of-thought reasoning,\n",
    "and role assignment. Effective prompt engineering can dramatically improve model performance without\n",
    "any fine-tuning. Studies show well-engineered prompts can improve accuracy by 20-40% on complex tasks.\n",
    "\n",
    "Vector databases store high-dimensional embeddings and enable fast similarity search. Systems like\n",
    "Pinecone, Weaviate, Chroma, and FAISS index millions of vectors and return nearest neighbors in\n",
    "milliseconds. This is essential for RAG pipelines where relevant chunks must be retrieved quickly\n",
    "from large knowledge bases at query time.\n",
    "\n",
    "Fine-tuning adapts a pretrained model to a specific domain or task by continuing training on a\n",
    "curated dataset. Techniques include full fine-tuning, LoRA (Low-Rank Adaptation), QLoRA, and\n",
    "instruction tuning. Fine-tuning can significantly improve performance on specialized tasks\n",
    "but requires labeled data, compute resources, and careful hyperparameter selection.\n",
    "\n",
    "Model evaluation requires careful benchmark design. Common metrics include BLEU and ROUGE for\n",
    "text generation, F1 score for information extraction, perplexity for language modeling, and\n",
    "human preference evaluation for open-ended generation. A robust evaluation suite should cover\n",
    "accuracy, fairness, robustness, and cost efficiency across diverse task types.\n",
    "\"\"\".strip()\n",
    "\n",
    "# ── Chunking Function ─────────────────────────────────────────────────────────\n",
    "def chunk_document(text: str, chunk_size_tokens: int, overlap_tokens: int = 50) -> list[str]:\n",
    "    \"\"\"Split document into overlapping chunks of approximately chunk_size_tokens tokens.\"\"\"\n",
    "    words = text.split()\n",
    "    # ~0.75 words per token (GPT tokenizer approximation)\n",
    "    words_per_chunk = int(chunk_size_tokens * 0.75)\n",
    "    overlap_words   = int(overlap_tokens * 0.75)\n",
    "    chunks, start   = [], 0\n",
    "    while start < len(words):\n",
    "        end = min(start + words_per_chunk, len(words))\n",
    "        chunks.append(\" \".join(words[start:end]))\n",
    "        start += words_per_chunk - overlap_words\n",
    "    return chunks\n",
    "\n",
    "# ── Chunk Configuration ───────────────────────────────────────────────────────\n",
    "CHUNK_SIZES   = [200, 500, 1000]          # tokens\n",
    "TOP_K         = 3                          # chunks retrieved per query\n",
    "RAG_QUESTION  = \"What is RAG and how does it reduce hallucination?\"\n",
    "RAG_EXPECTED_KEYWORDS = [\"retrieval\", \"retrieval-augmented\", \"hallucination\", \"context\", \"grounded\", \"knowledge\"]\n",
    "\n",
    "# Preview chunks for each size\n",
    "print(f\"Source document: ~{estimate_tokens(SOURCE_DOCUMENT)} tokens\\n\")\n",
    "for cs in CHUNK_SIZES:\n",
    "    chunks = chunk_document(SOURCE_DOCUMENT, cs)\n",
    "    avg_tok = int(np.mean([estimate_tokens(c) for c in chunks]))\n",
    "    print(f\"  Chunk size {cs:>4} tokens → {len(chunks)} chunks, avg {avg_tok} tokens each\")\n",
    "print(f\"\\nTop-k retrieval : {TOP_K} chunks per query\")\n",
    "print(f\"RAG question    : {RAG_QUESTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RAG Chunk Size Experiment Runner ─────────────────────────────────────────\n",
    "\n",
    "def simple_retrieve(chunks: list[str], question: str, top_k: int) -> list[str]:\n",
    "    \"\"\"Keyword-overlap retrieval (simulates vector similarity search).\"\"\"\n",
    "    q_words = set(question.lower().split())\n",
    "    scored  = [(sum(1 for w in c.lower().split() if w in q_words), c) for c in chunks]\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [c for _, c in scored[:top_k]]\n",
    "\n",
    "def build_rag_prompt(retrieved_chunks: list[str], question: str) -> str:\n",
    "    context = \"\\n\\n\".join(f\"[Chunk {i+1}]\\n{c}\" for i, c in enumerate(retrieved_chunks))\n",
    "    return f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer concisely based only on the context.\"\n",
    "\n",
    "rag_results = []\n",
    "\n",
    "print(\"Running RAG Chunk Size Experiment...\")\n",
    "print(f\"{'Chunk Size':>12} {'Model':>28} {'Input Tok':>10} {'Cost (µ$)':>10} {'Accuracy':>10} {'Latency(s)':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    chunks    = chunk_document(SOURCE_DOCUMENT, chunk_size)\n",
    "    retrieved = simple_retrieve(chunks, RAG_QUESTION, TOP_K)\n",
    "    prompt    = build_rag_prompt(retrieved, RAG_QUESTION)\n",
    "    context_tokens = estimate_tokens(prompt)\n",
    "\n",
    "    for model in runnable_models:\n",
    "        result = run_model(model, prompt, \"rag\")\n",
    "\n",
    "        # Score accuracy against RAG keywords\n",
    "        output_lower = (result[\"output\"]).lower()\n",
    "        hits     = sum(1 for kw in RAG_EXPECTED_KEYWORDS if kw in output_lower)\n",
    "        accuracy = round(hits / len(RAG_EXPECTED_KEYWORDS), 3)\n",
    "        if DEMO_MODE:\n",
    "            char     = MODEL_CHARACTERISTICS[model]\n",
    "            accuracy = min(1.0, max(0.0,\n",
    "                BASE_ACCURACY[\"rag\"] + char[\"accuracy_boost\"] + np.random.normal(0, 0.04)\n",
    "            ))\n",
    "\n",
    "        cost_micro = result[\"cost\"] * 1e6\n",
    "        rag_results.append({\n",
    "            \"chunk_size\":    chunk_size,\n",
    "            \"model\":         model,\n",
    "            \"provider\":      MODEL_REGISTRY[model][\"provider\"],\n",
    "            \"tier\":          MODEL_REGISTRY[model][\"tier\"],\n",
    "            \"num_chunks\":    len(chunks),\n",
    "            \"retrieved_k\":   TOP_K,\n",
    "            \"input_tokens\":  result[\"input_tokens\"],\n",
    "            \"output_tokens\": result[\"output_tokens\"],\n",
    "            \"total_tokens\":  result[\"total_tokens\"],\n",
    "            \"cost_usd\":      result[\"cost\"],\n",
    "            \"cost_micro\":    cost_micro,\n",
    "            \"latency\":       result[\"latency\"],\n",
    "            \"accuracy\":      round(accuracy, 4),\n",
    "        })\n",
    "        print(f\"{chunk_size:>12} {model:>28} {result['input_tokens']:>10} {cost_micro:>10.4f} {accuracy:>10.4f} {result['latency']:>12.3f}\")\n",
    "\n",
    "rag_df = pd.DataFrame(rag_results)\n",
    "print(f\"\\nTotal runs: {len(rag_df)}  ({len(CHUNK_SIZES)} chunk sizes × {len(runnable_models)} models)\")\n",
    "rag_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb405f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RAG Chunk Size Visualization ─────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle(\"RAG Chunk Size vs Cost, Accuracy & Token Usage\", fontsize=16, fontweight='bold', color='white')\n",
    "\n",
    "chunk_agg = rag_df.groupby([\"chunk_size\", \"model\"]).agg(\n",
    "    avg_input_tokens = (\"input_tokens\", \"mean\"),\n",
    "    avg_cost_micro   = (\"cost_micro\",   \"mean\"),\n",
    "    avg_accuracy     = (\"accuracy\",     \"mean\"),\n",
    "    avg_latency      = (\"latency\",      \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "models_rag = list(rag_df[\"model\"].unique())\n",
    "color_map  = {m: PALETTE[i % len(PALETTE)] for i, m in enumerate(models_rag)}\n",
    "\n",
    "# ── Panel 1: Input Tokens vs Chunk Size ──────────────────────────────────────\n",
    "ax = axes[0]\n",
    "for model in models_rag:\n",
    "    d = chunk_agg[chunk_agg[\"model\"] == model]\n",
    "    ax.plot(d[\"chunk_size\"], d[\"avg_input_tokens\"], 'o-',\n",
    "            color=color_map[model], label=model, lw=2, markersize=7)\n",
    "ax.set_xlabel(\"Chunk Size (tokens)\")\n",
    "ax.set_ylabel(\"Avg Input Tokens per Request\")\n",
    "ax.set_title(\"📈 Input Tokens vs Chunk Size\")\n",
    "ax.set_xticks(CHUNK_SIZES)\n",
    "ax.legend(fontsize=7, loc='upper left')\n",
    "ax.grid(True)\n",
    "\n",
    "# ── Panel 2: Accuracy vs Chunk Size ──────────────────────────────────────────\n",
    "ax2 = axes[1]\n",
    "for model in models_rag:\n",
    "    d = chunk_agg[chunk_agg[\"model\"] == model]\n",
    "    ax2.plot(d[\"chunk_size\"], d[\"avg_accuracy\"], 'o-',\n",
    "             color=color_map[model], label=model, lw=2, markersize=7)\n",
    "ax2.set_xlabel(\"Chunk Size (tokens)\")\n",
    "ax2.set_ylabel(\"Avg Accuracy Score\")\n",
    "ax2.set_title(\"🎯 Accuracy vs Chunk Size\")\n",
    "ax2.set_xticks(CHUNK_SIZES)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.legend(fontsize=7, loc='lower right')\n",
    "ax2.grid(True)\n",
    "\n",
    "# ── Panel 3: Cost vs Accuracy bubble (sized by chunk size) ───────────────────\n",
    "ax3 = axes[2]\n",
    "for _, row in chunk_agg.iterrows():\n",
    "    color  = color_map[row[\"model\"]]\n",
    "    size   = (row[\"chunk_size\"] / 10)          # 200→20, 500→50, 1000→100\n",
    "    ax3.scatter(row[\"avg_cost_micro\"], row[\"avg_accuracy\"],\n",
    "                s=size * 4, color=color, alpha=0.75,\n",
    "                edgecolors='white', linewidth=0.8, zorder=5)\n",
    "\n",
    "# Legend: chunk sizes as bubble sizes\n",
    "for cs in CHUNK_SIZES:\n",
    "    ax3.scatter([], [], s=(cs/10)*4, color='#aaaaaa',\n",
    "                label=f\"{cs} token chunks\", alpha=0.7, edgecolors='white')\n",
    "\n",
    "ax3.set_xlabel(\"Avg Cost per Request (µ$)\")\n",
    "ax3.set_ylabel(\"Avg Accuracy Score\")\n",
    "ax3.set_title(\"💰 Cost vs Accuracy\\n(bubble size = chunk size)\")\n",
    "ax3.legend(fontsize=8, title=\"Chunk Size\", loc='lower right')\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'chart8_rag_chunk_experiment.png'),\n",
    "            dpi=150, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "\n",
    "# ── Finding Summary ───────────────────────────────────────────────────────────\n",
    "print(\"\\n── RAG Chunk Experiment Findings ─────────────────────────────────────\")\n",
    "chunk_summary = rag_df.groupby(\"chunk_size\").agg(\n",
    "    avg_input_tokens = (\"input_tokens\", \"mean\"),\n",
    "    avg_accuracy     = (\"accuracy\",     \"mean\"),\n",
    "    avg_cost_micro   = (\"cost_micro\",   \"mean\"),\n",
    ").round(3)\n",
    "chunk_summary[\"tokens_per_accuracy\"] = (chunk_summary[\"avg_input_tokens\"] / chunk_summary[\"avg_accuracy\"]).round(1)\n",
    "print(chunk_summary.to_string())\n",
    "\n",
    "best_chunk = chunk_summary[\"tokens_per_accuracy\"].idxmin()\n",
    "print(f\"\\n✅ Most token-efficient chunk size : {best_chunk} tokens\")\n",
    "print(f\"   (lowest tokens needed per unit of accuracy)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
